Data can be saved back to HDFS in different file formats

  RDD can be saved into text file or sequence file
  Data Frames can be saved into other file formats

Saving RDD â€“ Text file format

RDD have below APIs to save data into different file formats

	saveAsTextFile (most important and covered here)
	saveAsSequenceFile
	saveAsNewAPIHadoopFile
	saveAsObjectFile


scala> val orders = sc.textFile("/public/retail_db/orders")
18/07/28 20:37:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 240.3 KB, free 366.1 MB)
18/07/28 20:37:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 366.0 MB)
18/07/28 20:37:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.105:42155 (size: 23.3 KB, free: 366.3 MB)
18/07/28 20:37:33 INFO SparkContext: Created broadcast 0 from textFile at <console>:24
orders: org.apache.spark.rdd.RDD[String] = /public/retail_db/orders MapPartitionsRDD[1] at textFile at <console>:24

scala> val orderCountByStatus = orders.
     | map(order => (order.split(",")(3),1)).
     | countByKey
18/07/28 20:38:25 INFO FileInputFormat: Total input paths to process : 1
18/07/28 20:38:25 INFO SparkContext: Starting job: countByKey at <console>:27
18/07/28 20:38:25 INFO DAGScheduler: Registering RDD 3 (countByKey at <console>:27)
18/07/28 20:38:25 INFO DAGScheduler: Got job 0 (countByKey at <console>:27) with 2 output partitions
18/07/28 20:38:25 INFO DAGScheduler: Final stage: ResultStage 1 (countByKey at <console>:27)
18/07/28 20:38:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
18/07/28 20:38:25 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
18/07/28 20:38:25 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at countByKey at <console>:27), which has no missing parents
18/07/28 20:38:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 366.0 MB)
18/07/28 20:38:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 366.0 MB)
18/07/28 20:38:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.105:42155 (size: 2.8 KB, free: 366.3 MB)
18/07/28 20:38:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039
18/07/28 20:38:25 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at countByKey at <console>:27) (first 15 tasks are for partitions Vector(0, 1))
18/07/28 20:38:25 INFO YarnScheduler: Adding task set 0.0 with 2 tasks
18/07/28 20:38:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, raj-Lenovo-Y50-70, executor 1, partition 0, NODE_LOCAL, 7904 bytes)
18/07/28 20:38:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on raj-Lenovo-Y50-70:46047 (size: 2.8 KB, free: 93.3 MB)
18/07/28 20:38:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on raj-Lenovo-Y50-70:46047 (size: 23.3 KB, free: 93.3 MB)
18/07/28 20:38:27 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, raj-Lenovo-Y50-70, executor 1, partition 1, NODE_LOCAL, 7904 bytes)
18/07/28 20:38:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1478 ms on raj-Lenovo-Y50-70 (executor 1) (1/2)
18/07/28 20:38:27 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 152 ms on raj-Lenovo-Y50-70 (executor 1) (2/2)
18/07/28 20:38:27 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/07/28 20:38:27 INFO DAGScheduler: ShuffleMapStage 0 (countByKey at <console>:27) finished in 1.759 s
18/07/28 20:38:27 INFO DAGScheduler: looking for newly runnable stages
18/07/28 20:38:27 INFO DAGScheduler: running: Set()
18/07/28 20:38:27 INFO DAGScheduler: waiting: Set(ResultStage 1)
18/07/28 20:38:27 INFO DAGScheduler: failed: Set()
18/07/28 20:38:27 INFO DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[4] at countByKey at <console>:27), which has no missing parents
18/07/28 20:38:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 366.0 MB)
18/07/28 20:38:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1975.0 B, free 366.0 MB)
18/07/28 20:38:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.105:42155 (size: 1975.0 B, free: 366.3 MB)
18/07/28 20:38:27 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039
18/07/28 20:38:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[4] at countByKey at <console>:27) (first 15 tasks are for partitions Vector(0, 1))
18/07/28 20:38:27 INFO YarnScheduler: Adding task set 1.0 with 2 tasks
18/07/28 20:38:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, raj-Lenovo-Y50-70, executor 1, partition 0, NODE_LOCAL, 7660 bytes)
18/07/28 20:38:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on raj-Lenovo-Y50-70:46047 (size: 1975.0 B, free: 93.3 MB)
18/07/28 20:38:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.1.105:42302
18/07/28 20:38:27 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, raj-Lenovo-Y50-70, executor 1, partition 1, NODE_LOCAL, 7660 bytes)
18/07/28 20:38:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 101 ms on raj-Lenovo-Y50-70 (executor 1) (1/2)
18/07/28 20:38:27 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 31 ms on raj-Lenovo-Y50-70 (executor 1) (2/2)
18/07/28 20:38:27 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/07/28 20:38:27 INFO DAGScheduler: ResultStage 1 (countByKey at <console>:27) finished in 0.228 s
18/07/28 20:38:27 INFO DAGScheduler: Job 0 finished: countByKey at <console>:27, took 2.169430 s
orderCountByStatus: scala.collection.Map[String,Long] = Map(PAYMENT_REVIEW -> 729, CLOSED -> 7556, SUSPECTED_FRAUD -> 1558, PROCESSING -> 8275, COMPLETE -> 22899, PENDING -> 7610, PENDING_PAYMENT -> 15030, ON_HOLD -> 3798, CANCELED -> 1428)

scala> // instead of using countByKey which is ascala collection not a RDD ,we can reduceByKey

scala> val orderCountByStatus = orders.
     | map(order => (order.split(",")(3),1)).
     | reduceByKey((total,element) => total+element)
orderCountByStatus: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at <console>:27

scala> orderCountByStatus.
++                         countByValue            intersection             productElement                       subtract          
aggregate                  countByValueApprox      isCheckpointed           productIterator                      subtractByKey     
aggregateByKey             dependencies            isEmpty                  productPrefix                        take              
cache                      distinct                iterator                 randomSplit                          takeAsync         
canEqual                   filter                  join                     reduce                               takeOrdered       
cartesian                  filterByRange           keyBy                    reduceByKey                          takeSample        
checkpoint                 first                   keys                     reduceByKeyLocally                   toDF              
coalesce                   flatMap                 leftOuterJoin            repartition                          toDS              
cogroup                    flatMapValues           localCheckpoint          repartitionAndSortWithinPartitions   toDebugString     
collect                    fold                    lookup                   rightOuterJoin                       toJavaRDD         
collectAsMap               foldByKey               map                      sample                               toLocalIterator   
collectAsync               foreach                 mapPartitions            sampleByKey                          toString          
combineByKey               foreachAsync            mapPartitionsWithIndex   sampleByKeyExact                     top               
combineByKeyWithClassTag   foreachPartition        mapValues                saveAsHadoopDataset                  treeAggregate     
compute                    foreachPartitionAsync   max                      saveAsHadoopFile                     treeReduce        
context                    fullOuterJoin           min                      saveAsNewAPIHadoopDataset            union             
copy                       getCheckpointFile       name                     saveAsNewAPIHadoopFile               unpersist         
count                      getNumPartitions        partitionBy              saveAsObjectFile                     values            
countApprox                getStorageLevel         partitioner              saveAsSequenceFile                   zip               
countApproxDistinct        glom                    partitions               saveAsTextFile                       zipPartitions     
countApproxDistinctByKey   groupBy                 persist                  setName                              zipWithIndex      
countAsync                 groupByKey              pipe                     sortBy                               zipWithUniqueId   
countByKey                 groupWith               preferredLocations       sortByKey                                              
countByKeyApprox           id                      productArity             sparkContext                                           

scala> orderCountByStatus.
!=                         countByKey              groupByKey               persist                              subtract          
##                         countByKeyApprox        groupWith                pipe                                 subtractByKey     
+                          countByValue            hashCode                 preferredLocations                   synchronized      
++                         countByValueApprox      id                       productArity                         take              
->                         dependencies            intersection             productElement                       takeAsync         
==                         distinct                isCheckpointed           productIterator                      takeOrdered       
aggregate                  ensuring                isEmpty                  productPrefix                        takeSample        
aggregateByKey             eq                      isInstanceOf             randomSplit                          toDF              
asInstanceOf               equals                  iterator                 reduce                               toDS              
cache                      filter                  join                     reduceByKey                          toDebugString     
canEqual                   filterByRange           keyBy                    reduceByKeyLocally                   toJavaRDD         
cartesian                  first                   keys                     repartition                          toLocalIterator   
checkpoint                 flatMap                 leftOuterJoin            repartitionAndSortWithinPartitions   toString          
coalesce                   flatMapValues           localCheckpoint          rightOuterJoin                       top               
cogroup                    fold                    lookup                   sample                               treeAggregate     
collect                    foldByKey               map                      sampleByKey                          treeReduce        
collectAsMap               foreach                 mapPartitions            sampleByKeyExact                     union             
collectAsync               foreachAsync            mapPartitionsWithIndex   saveAsHadoopDataset                  unpersist         
combineByKey               foreachPartition        mapValues                saveAsHadoopFile                     values            
combineByKeyWithClassTag   foreachPartitionAsync   max                      saveAsNewAPIHadoopDataset            wait              
compute                    formatted               min                      saveAsNewAPIHadoopFile               zip               
context                    fullOuterJoin           name                     saveAsObjectFile                     zipPartitions     
copy                       getCheckpointFile       ne                       saveAsSequenceFile                   zipWithIndex      
count                      getClass                notify                   saveAsTextFile                       zipWithUniqueId   
countApprox                getNumPartitions        notifyAll                setName                              â†’                 
countApproxDistinct        getStorageLevel         partitionBy              sortBy                                                 
countApproxDistinctByKey   glom                    partitioner              sortByKey                                              
countAsync                 groupBy                 partitions               sparkContext                                           

scala> orderCountByStatus.sa
sample        sampleByKeyExact      saveAsHadoopFile            saveAsNewAPIHadoopFile   saveAsSequenceFile   
sampleByKey   saveAsHadoopDataset   saveAsNewAPIHadoopDataset   saveAsObjectFile         saveAsTextFile       

scala> orderCountByStatus.saveAsTextFile
   def saveAsTextFile(path: String,codec: Class[_ <: org.apache.hadoop.io.compress.CompressionCodec]): Unit   def saveAsTextFile(path: String): Unit

scala> orderCountByStatus.saveAsTextFile
   def saveAsTextFile(path: String,codec: Class[_ <: org.apache.hadoop.io.compress.CompressionCodec]): Unit   def saveAsTextFile(path: String): Unit

scala> orderCountByStatus.saveAsTextFile
   def saveAsTextFile(path: String,codec: Class[_ <: org.apache.hadoop.io.compress.CompressionCodec]): Unit   def saveAsTextFile(path: String): Unit

scala> orderCountByStatus.saveAsTextFile("/user/hduser/textFolder")
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:54310/user/hduser/textFolder already exists
  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)
  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:283)
  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)
  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1493)
  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)
  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1472)
  ... 49 elided

scala> orderCountByStatus.saveAsTextFile("/user/hduser/textFolder1")
18/07/28 20:43:06 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
18/07/28 20:43:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/07/28 20:43:06 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
18/07/28 20:43:06 INFO DAGScheduler: Registering RDD 5 (map at <console>:26)
18/07/28 20:43:06 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions
18/07/28 20:43:06 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:78)
18/07/28 20:43:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
18/07/28 20:43:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
18/07/28 20:43:06 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at map at <console>:26), which has no missing parents
18/07/28 20:43:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.6 KB, free 366.0 MB)
18/07/28 20:43:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.7 KB, free 366.0 MB)
18/07/28 20:43:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.105:42155 (size: 2.7 KB, free: 366.3 MB)
18/07/28 20:43:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1039
18/07/28 20:43:06 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at map at <console>:26) (first 15 tasks are for partitions Vector(0, 1))
18/07/28 20:43:06 INFO YarnScheduler: Adding task set 2.0 with 2 tasks
18/07/28 20:43:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, raj-Lenovo-Y50-70, executor 1, partition 0, NODE_LOCAL, 7904 bytes)
18/07/28 20:43:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on raj-Lenovo-Y50-70:46047 (size: 2.7 KB, free: 93.3 MB)
18/07/28 20:43:06 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, raj-Lenovo-Y50-70, executor 1, partition 1, NODE_LOCAL, 7904 bytes)
18/07/28 20:43:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 134 ms on raj-Lenovo-Y50-70 (executor 1) (1/2)
18/07/28 20:43:06 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 71 ms on raj-Lenovo-Y50-70 (executor 1) (2/2)
18/07/28 20:43:06 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/07/28 20:43:06 INFO DAGScheduler: ShuffleMapStage 2 (map at <console>:26) finished in 0.236 s
18/07/28 20:43:06 INFO DAGScheduler: looking for newly runnable stages
18/07/28 20:43:06 INFO DAGScheduler: running: Set()
18/07/28 20:43:06 INFO DAGScheduler: waiting: Set(ResultStage 3)
18/07/28 20:43:06 INFO DAGScheduler: failed: Set()
18/07/28 20:43:06 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at saveAsTextFile at <console>:26), which has no missing parents
18/07/28 20:43:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 72.6 KB, free 366.0 MB)
18/07/28 20:43:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 26.4 KB, free 365.9 MB)
18/07/28 20:43:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.105:42155 (size: 26.4 KB, free: 366.2 MB)
18/07/28 20:43:06 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1039
18/07/28 20:43:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at saveAsTextFile at <console>:26) (first 15 tasks are for partitions Vector(0, 1))
18/07/28 20:43:06 INFO YarnScheduler: Adding task set 3.0 with 2 tasks
18/07/28 20:43:06 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, raj-Lenovo-Y50-70, executor 1, partition 0, NODE_LOCAL, 7660 bytes)
18/07/28 20:43:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on raj-Lenovo-Y50-70:46047 (size: 26.4 KB, free: 93.2 MB)
18/07/28 20:43:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.1.105:42302
18/07/28 20:43:06 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, raj-Lenovo-Y50-70, executor 1, partition 1, NODE_LOCAL, 7660 bytes)
18/07/28 20:43:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 251 ms on raj-Lenovo-Y50-70 (executor 1) (1/2)
18/07/28 20:43:06 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 67 ms on raj-Lenovo-Y50-70 (executor 1) (2/2)
18/07/28 20:43:06 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/07/28 20:43:06 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:78) finished in 0.428 s
18/07/28 20:43:06 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:78, took 0.668624 s
18/07/28 20:43:07 INFO SparkHadoopWriter: Job job_20180728204306_0008 committed.





Saving RDD â€“ Compression

Compression can reduce storage requirements significantly. It is important to understand how to compress the output while saving into HDFS.

	Only those compression codecs defined in /etc/hadoop/conf/core-site.xml can be used
	We can use additional argument to pass compression codec while saving data using one of the APIs such as saveAsTextFile
	To read compressed data we do not need to use any codec. As long as the files are compressed in supported formats, we just need to use APIs 		such as sc.textFile to read the data (compressed or not does not matter)


Saving data in other formats

Below format can be used only for Data frames not RDD.We can convert data into data frame and save data in any of these formats

	json (as shown below)
	parquet
	orc
	text
	csv (3rd party plugin)
	avro (3rd party plugin, but cloudera clusters are well integrated with avro)
There are 2 APIs which can be used to save data frames

save â€“ takes 2 arguments, path and file format
write â€“ provides interfaces such as json to save data in the path specified








