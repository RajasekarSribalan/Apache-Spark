Spark is a Distributed Computing framework
It is not a programming language
It's core level api are Transformation and aggregation
High level modules are Data Frames/Sql,Streams and more.
Well Integerated with Python and Scala
Spark Uses HDFS Api to deal with file system
It doesnot have undelying file system'
Core Spark and Spark SQL is required for CCA


**** Spark enable distributed data processing through functional transformation on distributed collections of data. ***
Keeping data in memory
*** Distributed shared memory

spark-shell --master yarn --conf spark.ui.port=12654

RDD:

RDD is a read only ,partitioned collection of records.
Load several datasets into memory and run ad-hoc query on them
Fault tolerant
Persist immediate results in memory

Limitations of RDD:

It is designed for batch application

How it works?

1)When we create a spark application is called as a driver program.The spark application which has the main method or triggering point is calld the driver program

2)Spark context should be created to run the spark app in the  spark cluster.It will be treated as a gateweay between your driver and the cluster.It will communicate with the CLuster Manager.(E..g Yarn,Mesos,Spark master)

3)Once the application is available for the cluster manager which has the information of all resources(CPU,RAM,Disc of worker nodes),

4)There is no concept of namenode in the spark

5)Executors will create tasks and the task will work on the data

6)If we want to persist the data ,then cache can be used.

Worker node:
	Node thar run the application program in cluster.
Executor :
 Process Launched on worker node,that runs the Tasks

Task:
 basic unit of the work

****** Running Spark application in local mode *****
sc.parallelize(1 to 100)
Only one Task is executed and only one stage and one executor

****** Running Spark application in Yarn mode *****

sc.parallelize(1 to 100)
Two tasks are created
2 Exectuors for 2 tasks
2 executor for driver

Note: Why 2 tasks? It is because number of tasks are dependent upon on number of partitions of a RDD
a.partitions.length

Int = 2

Default by yarn,two partitions are allocated in this machine

When it is run in local mode,by default  one partition will be used hence only one task

It can be overriden by --master local[2]
So 2 means number of cores

So,number of paritions means num of cores required to process

If we are not sure of number of paritions,we can give --master local[*]

It ran 8 task because of 8 cores but only one executor

In yarn mode,

--master yarn

I parallelized 8 partitions.

sc.parallelize(1 to 100,8)

2 executors were created
8 tasks were executed.4 tasks on each executer

If i set, sc.parallelize(1 to 100,9)

2 executors were created
10 tasks were executed.5 tasks on each executer



Memory management (Important)
 By default,2 Executors will be created for Yarn

Executor memory -executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).
num executors  --num-executors NUM         Number of executors to launch (Default: 2)

We need to increaed or decrease based on the size of data.

hadoop fs -du -s -h /relatil_db

How to get details about defaults - > Go to Sparl/conf/Spark-env.sh. This file will have default values



****RDD

In-memory
distributed collection
Resilient - Data is being processed by a task and if a task is failed,RDD will be recomputed by another task

RDD can be created in two ways.Once is through sc.textFile -> reading the text file from HDFS and another way is through paralleize a collection

Our data set hadoop fs -tail /public/retail_db/orders/part-00000

**************Reading data using SparkContext

******** Create RDD from HDFS

1)Launch Spark-shell

spark-shell --master yarn \
 --num-executors 1 \
 --executor-memory 512M


2)sc.textFile("") ->Only directory name is enough as all the files under the directory will be processed.

//RDD from files in HDFS
3) val fileRDD = sc.textFile("/public/retail_db/orders/")
4) fileRDD.first
	res1: String = 1,2013-07-25 00:00:00.0,11599,CLOSED
5) fileRDD.take(10)
	res2: Array[String] = Array(1,2013-07-25 00:00:00.0,11599,CLOSED, 2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT, 3,2013-07-25 00:00:00.0,12111,COMPLETE, 4,2013-07-25 00:00:00.0,8827,CLOSED, 5,2013-07-25 00:00:00.0,11318,COMPLETE, 6,2013-07-25 00:00:00.0,7130,COMPLETE, 7,2013-07-25 00:00:00.0,4530,COMPLETE, 8,2013-07-25 00:00:00.0,2911,PROCESSING, 9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT, 10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT)

******** Create RDD from local filesystem

We cannot use sc.textFile.We need to use scala FIle api to read the file from the local filesystem and then convert it as RDD using paraellize.

1)Launch Spark-shell

spark-shell --master yarn \
 --num-executors 1 \
 --executor-memory 512M



// RDD from files in local file system
2)val productsRaw = scala.io.Source.fromFile("/home/raj/Project/itversity_data/data/retail_db/orders/part-00000").getLines.toList
3)val products = sc.parallelize(productsRaw)
4)products.first
 res0: String = 1,2013-07-25 00:00:00.0,11599,CLOSED
5)products.take(3)
 res1: Array[String] = Array(1,2013-07-25 00:00:00.0,11599,CLOSED, 2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT, 3,2013-07-25 00:00:00.0,12111,COMPLETE)
6)products.count
 res2: Long = 68883


*******Previewing data using actions
Following are the operations on top of RDD that can be performed to preview the data

take
first
count
and more

To get the genertaed DAG -> orderRDD.toDebugString

val orders = sc.textFile("/public/retail_db/orders")

// Previewing data
orders.first
orders.take(10).foreach(println)
orders.count

// Use collect with care. 
// As it creates single threaded list from distributed RDD, 
// using collect on larger datasets can cause out of memory issues.
orders.collect.foreach(println) 



******Reading data using SQLContext
Spark Context has api only to read textfiles from local or HDFS.
To read Parquet,json or csv,we need to use SqlContext.

SQLContext have 2 APIs to read data of different file formats

load – typically takes 2 arguments, path and format
read – have interface for each of the file format (e.g.: read.json)
Following are the file formats supported

text
orc
parquet
json (example shown)
csv (3rd party plugin)
avro (3rd party plugin, but Cloudera clusters get by default)

-------------------
 val json = spark.read.json("/public/retail_db_json/orders/")
json.show;


val jsonRDD = spark.read.format("json").load("/public/retail_db_json/orders/")
jsonRDD.show;
----------------------


