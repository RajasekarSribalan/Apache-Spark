
Flume – Getting Started

Let us understand briefly about Flume.

    It is a data ingestion tool
    It can get log messages from log files, syslog and many other sources
    No design changes are required

Documentation and Simple Example

Flume require

    One or more sources
    One or more sinks
    One channel for each sink

In the simple example we will see flume agent configuration with

    One source – netcat web service
    One sink – logger
    One channel – memory


Go Flume user guide 1.6 and chekc the example.conf.

Create a directory and create file and copy the content of example.conf to it.

and run flume using below command

flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console

Integration to HDFS – Introduction

HDFS is one of the most common sinks to which data is pushed to by Flume agents. There are several properties that can be override while pushing data to HDFS.

    Path
    File Suffix
    File Prefix
    Roll Properties
    Timestamp
    and many more

Get web server logs from to HDFS using flume

source - exec (tail -f )
sink - hdfs sink
channel - memory or file


Setting up data

Let us see how we can set up data to get simulated log messages pushed to HDFS. We use gen_logs application to continuously generate log messages in /opt/gen_logs/logs/alert.log file.

Go to /opt/gen_logs/ for generating web server logs



******  Flume – Web Server logs to HDFS


Let us see how we can set up Flume agent to get data from web server logs into HDFS

*--Define Source – exec type

Let us read data from web server logs.

    Use exec type as source
    Issue tail -F command to continuously read messages from the simulated logs



wh.sources = ws
wh.sinks = k1
wh.channels = mem

# Describe/configure the source
wh.sources.ws.type = exec
wh.sources.ws.comman = tail -f /op/gen_logs/logs/access.log


*---Define Sink – HDFS type

Let us get started with HDFS sink, by defining

    Path
    Rest of the properties are defaults

# Describe the sink
wh.sinks.hd.type = hdfs
wh.sinks.hd.hdfs.path=hdfs://nn01.itversity.com:8020/user/rajasekar/flume_demo




===================================

# wshdfs.conf
# To get the data from web server logs to HDFS
wh.sources = ws
wh.sinks = hd
wh.channels = mem

# Describe/configure the source
wh.sources.ws.type = exec
wh.sources.ws.command = tail -F /opt/gen_logs/logs/access.log

# Describe the sink
wh.sinks.hd.type = hdfs
wh.sinks.hd.hdfs.path = hdfs://nn01.itversity.com:8020/user/dgadiraju/flume_demo

wh.sinks.hd.hdfs.filePrefix = FlumeDemo
wh.sinks.hd.hdfs.fileSuffix = .txt
wh.sinks.hd.hdfs.rollInterval = 120
wh.sinks.hd.hdfs.rollSize = 1048576
wh.sinks.hd.hdfs.rollCount = 100
wh.sinks.hd.hdfs.fileType = DataStream

# Use a channel which buffers events in memory
wh.channels.mem.type = memory
wh.channels.mem.capacity = 1000
wh.channels.mem.transactionCapacity = 100

# Bind the source and sink to the channel
wh.sources.ws.channels = mem
wh.sinks.hd.channel = mem


==================================================


Different Flume implementations

Flume agents can be configured using different flows

    Linear – Multiple flume agents integrated by avro sink of prior agent and avro source of later agent
    Consolidation – Multiple flume agents pointing to different web server log sources will be consumed by one agent (again integrated by avro)
    Multiplex – Data from one source can be pushed to multiple sinks. There will be one channel for each of the sink.



