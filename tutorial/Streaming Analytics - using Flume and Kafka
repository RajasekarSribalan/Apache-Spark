
Flume – Getting Started

Let us understand briefly about Flume.

    It is a data ingestion tool
    It can get log messages from log files, syslog and many other sources
    No design changes are required

Documentation and Simple Example

Flume require

    One or more sources
    One or more sinks
    One channel for each sink

In the simple example we will see flume agent configuration with

    One source – netcat web service
    One sink – logger
    One channel – memory


Go Flume user guide 1.6 and chekc the example.conf.

Create a directory and create file and copy the content of example.conf to it.

and run flume using below command

flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console

Integration to HDFS – Introduction

HDFS is one of the most common sinks to which data is pushed to by Flume agents. There are several properties that can be override while pushing data to HDFS.

    Path
    File Suffix
    File Prefix
    Roll Properties
    Timestamp
    and many more

Get web server logs from to HDFS using flume

source - exec (tail -f )
sink - hdfs sink
channel - memory or file


Setting up data

Let us see how we can set up data to get simulated log messages pushed to HDFS. We use gen_logs application to continuously generate log messages in /opt/gen_logs/logs/alert.log file.

Go to /opt/gen_logs/ for generating web server logs



******  Flume – Web Server logs to HDFS


Let us see how we can set up Flume agent to get data from web server logs into HDFS

*--Define Source – exec type

Let us read data from web server logs.

    Use exec type as source
    Issue tail -F command to continuously read messages from the simulated logs



wh.sources = ws
wh.sinks = k1
wh.channels = mem

# Describe/configure the source
wh.sources.ws.type = exec
wh.sources.ws.comman = tail -f /op/gen_logs/logs/access.log


*---Define Sink – HDFS type

Let us get started with HDFS sink, by defining

    Path
    Rest of the properties are defaults

# Describe the sink
wh.sinks.hd.type = hdfs
wh.sinks.hd.hdfs.path=hdfs://nn01.itversity.com:8020/user/rajasekar/flume_demo




===================================

# wshdfs.conf
# To get the data from web server logs to HDFS
wh.sources = ws
wh.sinks = hd
wh.channels = mem

# Describe/configure the source
wh.sources.ws.type = exec
wh.sources.ws.command = tail -F /opt/gen_logs/logs/access.log

# Describe the sink
wh.sinks.hd.type = hdfs
wh.sinks.hd.hdfs.path = hdfs://nn01.itversity.com:8020/user/dgadiraju/flume_demo

wh.sinks.hd.hdfs.filePrefix = FlumeDemo
wh.sinks.hd.hdfs.fileSuffix = .txt
wh.sinks.hd.hdfs.rollInterval = 120
wh.sinks.hd.hdfs.rollSize = 1048576
wh.sinks.hd.hdfs.rollCount = 100
wh.sinks.hd.hdfs.fileType = DataStream

# Use a channel which buffers events in memory
wh.channels.mem.type = memory
wh.channels.mem.capacity = 1000
wh.channels.mem.transactionCapacity = 100

# Bind the source and sink to the channel
wh.sources.ws.channels = mem
wh.sinks.hd.channel = mem


==================================================


Different Flume implementations

Flume agents can be configured using different flows

    Linear – Multiple flume agents integrated by avro sink of prior agent and avro source of later agent
    Consolidation – Multiple flume agents pointing to different web server log sources will be consumed by one agent (again integrated by avro)
    Multiplex – Data from one source can be pushed to multiple sinks. There will be one channel for each of the sink.






****************** KAFKA *************************

Kafka is getting popular in ingesting data in streaming fashion from different sources to different targets due to several reasons.

    Highly reliable
    Highly scalable
    Robust APIs
    Easy to integrate

High level architecture

Kafka contains several components

    Zoo Keeper – to keep Kafka brokers up and running
    Kafka Broker – who manages topics
    Kafka Cluster contains multiple brokers

Kafka provide APIs for

    Producers – to publish messages into Kafka topic
    Consumers – to consume messages from Kafka topic
    Stream processors
    Connectors (such as Database)

Sample Commands – to validate

Kafka provides command line utilities to

    manage topics – create, list and delete
    publish messages to topic
    consume messages from topic

Make sure environment variables are set to use the commands.


kafka-topics.sh --create \
   --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
   --replication-factor 1 \
   --partitions 1 \
   --topic kafkademodg

kafka-topics.sh --list \
   --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
   --topic kafkademodg

kafka-console-producer.sh \
  --broker-list nn01.itversity.com:6667,nn02.itversity.com:6667,rm01.itversity.com:6667 \
  --topic kafkademodg

kafka-console-consumer.sh \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
  --topic kafkademodg \
--from-beginning

Anatomy of a topic

Kafka topic have important properties like

    partition – for scalability
    replication – for reliability

We can define number of partitions and replication factor while creating the topics. Consumers maintain offsets to understand till the position of messages that are read in each of the partition.

Anatomy of a topic

Kafka topic have important properties like

    partition – for scalability
    replication – for reliability

We can define number of partitions and replication factor while creating the topics. Consumers maintain offsets to understand till the position of messages that are read in each of the partition.


1.Topic is nothing but a file which captures stream of messages.
2.Publishers publish message to topic.
3.Consumers subscribe to the topic.
4.Topics can be partitioned for scalability.
5.Each topic partition can be cloned for reliabillity.
5.Offset - posotion of the last message consumers have read from each partition of the topic.
6.Consumer group can be created to facilitate multiple consumers read from same topic in co-ordinated fashion (offset is tracked at group-level).



Role of Kafka and Flume

Even though Kafka is increasingly used as part of the new applications, there is resistance for legacy applications. Hence in case of legacy applications, Flume and Kafka complement well.

    Read data from legacy applications using Flume
    Use Kafka sink to push messages to Kafka topic
    Consume messages from Kafka and process using technologies like Spark Streaming


Flume advantages - if exisitng code is up and running and logs are being logged which then needs to be analyzed ,then flume can be installed and flume gaent can be used to read 
the logs.So, there is no changes to application.

disadvantages -not reliable and not scalable



***************** 

Spark Streaming – Getting Started


Let us get started with Spark Streaming.

    Spark Streaming is a module which provide APIs to process streaming data
    It can be integrated with any streaming ingestion technology such as Flume, Kafka etc

Getting Started

Spark Streaming is used to process data in streaming fashion.

    It requires web service called StreamingContext
    Unlike SparkContext, StreamingContext runs perpetually processing data at regular intervals
    We cannot have multiple contexts running at same time, hence if there is running SparkContext we need to stop it before we launch StreamingContext



=============

import org.apache.spark.streaming._
import org.apache.spark.SparkConf
val conf = new SparkConf().setAppName("Spark").setMaster("yarn-client");
val ssc = new StreamingContext(conf,Seconds(10))

=============
